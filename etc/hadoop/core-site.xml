<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
	       <!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://mycluster</value>
	</property>

    <property>
        <name>ha.zookeeper.quorum</name>
        <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>
    </property>

    <!-- 指定hadoop数据的存储目录     
      官方配置文件中的配置项是hadoop.tmp.dir ,用来指定hadoop数据的存储目录,
	  此次配置用的hadoop.data.dir是自己定义的变量，
	  因为在hdfs-site.xml中会使用此配置的值来具体指定namenode 和 datanode存储数据的目录
    -->
    <property>
        <name>hadoop.data.dir</name>
        <value>/opt/app/hadoop-3.1.3/data</value>
    </property>

   <!-- 声明journalnode服务器存储目录-->
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>file://${hadoop.data.dir}/jn</value>
    </property>
	
	<property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
		<description>Size of read/write buffer used in SequenceFiles.</description>
    </property>

    <property>
        <name>hadoop.proxyuser.zhanghoumin.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.zhanghoumin.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>zhanghoumin</value>
    </property>
	
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>

    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>


<!-- 一下两个参数，是由于使用start-dfs.sh启动集群时，journalnode（3个）启动慢，而namenode（3个）启动时连接不上jn导致启动失败或者高可用不可用（实际情况是1个active，kill掉后其余俩还是standby），故调整这俩参数，使namenode在最大尝试次数和时间之前能 连上jn -->
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>100</value>
        <description>默认10。Indicates the number of retries a client will make to establish a server connection.</description>
    </property>
    <property>
        <name>ipc.client.connect.timeout</name>
        <value>5000</value>
        <description>默认1000ms。Indicates the number of milliseconds a client will wait for before retrying to establish a server connection.</description>
    </property>
</configuration>
